{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annina/miniconda3/envs/rdl_t3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/annina/miniconda3/envs/rdl_t3/lib/python3.8/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Conv1D\n",
    "import math\n",
    "\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annina/miniconda3/envs/rdl_t3/lib/python3.8/site-packages/datasets/load.py:1429: FutureWarning: The repository for allenai/winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/allenai/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a small Hugging Face dataset\n",
    "dataset = load_dataset(\"allenai/winogrande\", \"winogrande_xs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annina/miniconda3/envs/rdl_t3/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model initialization\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, using eos_token instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 uses custom conv1d layers which are just linear layers with a weight transpose\n",
    "# Therefore we can just convert them to standard linear layers to simplify the architecture\n",
    "for name, layer in model.named_modules():\n",
    "    if not isinstance(layer, Conv1D):\n",
    "        continue\n",
    "    parent = model.get_submodule(name[:name.rfind(\".\")])\n",
    "    has_bias = torch.any(layer.bias.data)\n",
    "    linear_layer = torch.nn.Linear(*layer.weight.shape, bias=has_bias)\n",
    "    with torch.no_grad():\n",
    "        linear_layer.weight.copy_(layer.weight.T)\n",
    "        if has_bias:\n",
    "            linear_layer.bias.copy_(layer.bias)\n",
    "    setattr(parent, name.split(\".\")[-1], linear_layer)\n",
    "\n",
    "# GPT2 also uses a merged weight matrix for qkv which nowadays is not really done anymore\n",
    "# While this is equivalent to having 3 separate weight matrices, it is more clear and easier to implement with LoRA to have 3 separate weight matrices\n",
    "# Therefore we will split the weight matrix into 3 separate weight matrices\n",
    "class AttentionProjections(nn.Module):\n",
    "    def __init__(self, merged_weight):\n",
    "        super().__init__()\n",
    "        dim = merged_weight.shape[1]\n",
    "        q, k, v = layer.weight.data.split(dim)\n",
    "        q_bias, k_bias, v_bias = layer.bias.data.split(dim)\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        with torch.no_grad():\n",
    "            self.q_proj.weight.copy_(q)\n",
    "            self.k_proj.weight.copy_(k)\n",
    "            self.v_proj.weight.copy_(v)\n",
    "            self.q_proj.bias.copy_(q_bias)\n",
    "            self.k_proj.bias.copy_(k_bias)\n",
    "            self.v_proj.bias.copy_(v_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        return torch.cat([q, k, v], dim=-1)\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    if \"c_attn\" in name:\n",
    "        parent = model.get_submodule(name[:name.rfind(\".\")])\n",
    "        setattr(parent, name.split(\".\")[-1], AttentionProjections(layer.weight.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "def tokenize_function(examples):\n",
    "    concatenated_examples = [s + \" \" + a for s, a in zip(examples[\"sentence\"], examples[\"answer\"])]\n",
    "    return tokenizer(concatenated_examples, padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Apply the function using map\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Implement a LoRA layer that adds a low-rank trainable matrix to the frozen weights.\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Task: Implement a LoRA layer that adds a low-rank trainable matrix to the frozen weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer, rank=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.base = base_layer\n",
    "\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        in_dim = base_layer.in_features\n",
    "        out_dim = base_layer.out_features\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.empty((rank, in_dim)))\n",
    "        self.lora_B = nn.Parameter(torch.empty((out_dim, rank)))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        lora_out = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "        return base_out + lora_out * self.scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lora_layer_forward(LoRALinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): AttentionProjections(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Replace all q, k, v, o layers with LoRA\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, AttentionProjections):\n",
    "        # Replace q_proj, k_proj, v_proj inside AttentionProjections\n",
    "        module.q_proj = LoRALinear(module.q_proj, rank=8, alpha=16)\n",
    "        module.k_proj = LoRALinear(module.k_proj, rank=8, alpha=16)\n",
    "        module.v_proj = LoRALinear(module.v_proj, rank=8, alpha=16)\n",
    "    elif \"attn.c_proj\" in name:\n",
    "        parent = model.get_submodule(name[:name.rfind(\".\")])\n",
    "        orig_layer = getattr(parent, name.split(\".\")[-1])\n",
    "        setattr(parent, name.split(\".\")[-1], LoRALinear(orig_layer, rank=8, alpha=16))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Check that the correct layers are LoRA layers\n",
    "test_lora_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Layer: transformer.h.0.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.0.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.0.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.0.attn.c_proj\n",
      "LoRA Layer: transformer.h.1.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.1.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.1.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.1.attn.c_proj\n",
      "LoRA Layer: transformer.h.2.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.2.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.2.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.2.attn.c_proj\n",
      "LoRA Layer: transformer.h.3.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.3.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.3.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.3.attn.c_proj\n",
      "LoRA Layer: transformer.h.4.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.4.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.4.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.4.attn.c_proj\n",
      "LoRA Layer: transformer.h.5.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.5.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.5.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.5.attn.c_proj\n",
      "LoRA Layer: transformer.h.6.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.6.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.6.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.6.attn.c_proj\n",
      "LoRA Layer: transformer.h.7.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.7.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.7.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.7.attn.c_proj\n",
      "LoRA Layer: transformer.h.8.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.8.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.8.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.8.attn.c_proj\n",
      "LoRA Layer: transformer.h.9.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.9.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.9.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.9.attn.c_proj\n",
      "LoRA Layer: transformer.h.10.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.10.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.10.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.10.attn.c_proj\n",
      "LoRA Layer: transformer.h.11.attn.c_attn.q_proj\n",
      "LoRA Layer: transformer.h.11.attn.c_attn.k_proj\n",
      "LoRA Layer: transformer.h.11.attn.c_attn.v_proj\n",
      "LoRA Layer: transformer.h.11.attn.c_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRALinear):\n",
    "        print(\"LoRA Layer:\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: ensure gradients are only enabled for LoRA parameters\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable gradients only for LoRA A and B\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRALinear):\n",
    "        module.lora_A.requires_grad = True\n",
    "        module.lora_B.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainierbar: transformer.h.0.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.0.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.0.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.0.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.0.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.0.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.0.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.0.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.1.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.1.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.1.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.1.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.1.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.1.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.1.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.1.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.2.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.2.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.2.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.2.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.2.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.2.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.2.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.2.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.3.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.3.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.3.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.3.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.3.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.3.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.3.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.3.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.4.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.4.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.4.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.4.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.4.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.4.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.4.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.4.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.5.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.5.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.5.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.5.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.5.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.5.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.5.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.5.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.6.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.6.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.6.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.6.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.6.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.6.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.6.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.6.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.7.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.7.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.7.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.7.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.7.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.7.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.7.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.7.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.8.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.8.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.8.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.8.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.8.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.8.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.8.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.8.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.9.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.9.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.9.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.9.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.9.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.9.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.9.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.9.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.10.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.10.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.10.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.10.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.10.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.10.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.10.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.10.attn.c_proj.lora_B\n",
      "Trainierbar: transformer.h.11.attn.c_attn.q_proj.lora_A\n",
      "Trainierbar: transformer.h.11.attn.c_attn.q_proj.lora_B\n",
      "Trainierbar: transformer.h.11.attn.c_attn.k_proj.lora_A\n",
      "Trainierbar: transformer.h.11.attn.c_attn.k_proj.lora_B\n",
      "Trainierbar: transformer.h.11.attn.c_attn.v_proj.lora_A\n",
      "Trainierbar: transformer.h.11.attn.c_attn.v_proj.lora_B\n",
      "Trainierbar: transformer.h.11.attn.c_proj.lora_A\n",
      "Trainierbar: transformer.h.11.attn.c_proj.lora_B\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"Trainierbar:\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Check that only LoRA parameters are trainable\n",
    "# Adjust the lora_param_names to the actual parameter names used in your LoRA implementation\n",
    "test_only_lora_trainable(model, lora_param_names=[\"lora_A\", \"lora_B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.741665840148926\n",
      "Step 1, Loss: 10.323408126831055\n",
      "Step 2, Loss: 9.493453979492188\n",
      "Step 3, Loss: 9.08095645904541\n",
      "Step 4, Loss: 7.785666465759277\n"
     ]
    }
   ],
   "source": [
    "# Simple Training Loop (Few Steps)\n",
    "# You should see the loss go down\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step >= 5:  # Run for a few steps only\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
