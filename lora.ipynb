{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Conv1D\n",
    "\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small Hugging Face dataset\n",
    "dataset = load_dataset(\"allenai/winogrande\", \"winogrande_xs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model initialization\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, using eos_token instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 uses custom conv1d layers which are just linear layers with a weight transpose\n",
    "# Therefore we can just convert them to standard linear layers to simplify the architecture\n",
    "for name, layer in model.named_modules():\n",
    "    if not isinstance(layer, Conv1D):\n",
    "        continue\n",
    "    parent = model.get_submodule(name[:name.rfind(\".\")])\n",
    "    has_bias = torch.any(layer.bias.data)\n",
    "    linear_layer = torch.nn.Linear(*layer.weight.shape, bias=has_bias)\n",
    "    with torch.no_grad():\n",
    "        linear_layer.weight.copy_(layer.weight.T)\n",
    "        if has_bias:\n",
    "            linear_layer.bias.copy_(layer.bias)\n",
    "    setattr(parent, name.split(\".\")[-1], linear_layer)\n",
    "\n",
    "# GPT2 also uses a merged weight matrix for qkv which nowadays is not really done anymore\n",
    "# While this is equivalent to having 3 separate weight matrices, it is more clear and easier to implement with LoRA to have 3 separate weight matrices\n",
    "# Therefore we will split the weight matrix into 3 separate weight matrices\n",
    "class AttentionProjections(nn.Module):\n",
    "    def __init__(self, merged_weight):\n",
    "        super().__init__()\n",
    "        dim = merged_weight.shape[1]\n",
    "        q, k, v = layer.weight.data.split(dim)\n",
    "        q_bias, k_bias, v_bias = layer.bias.data.split(dim)\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        with torch.no_grad():\n",
    "            self.q_proj.weight.copy_(q)\n",
    "            self.k_proj.weight.copy_(k)\n",
    "            self.v_proj.weight.copy_(v)\n",
    "            self.q_proj.bias.copy_(q_bias)\n",
    "            self.k_proj.bias.copy_(k_bias)\n",
    "            self.v_proj.bias.copy_(v_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        return torch.cat([q, k, v], dim=-1)\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    if \"c_attn\" in name:\n",
    "        parent = model.get_submodule(name[:name.rfind(\".\")])\n",
    "        setattr(parent, name.split(\".\")[-1], AttentionProjections(layer.weight.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "def tokenize_function(examples):\n",
    "    concatenated_examples = [s + \" \" + a for s, a in zip(examples[\"sentence\"], examples[\"answer\"])]\n",
    "    return tokenizer(concatenated_examples, padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Apply the function using map\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Implement a LoRA layer that adds a low-rank trainable matrix to the frozen weights.\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Task: Implement a LoRA layer that adds a low-rank trainable matrix to the frozen weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer, rank=4):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lora_layer_forward(LoRALinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Replace all q, k, v, o layers with LoRA\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Check that the correct layers are LoRA layers\n",
    "test_lora_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: ensure gradients are only enabled for LoRA parameters\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Check that only LoRA parameters are trainable\n",
    "# Adjust the lora_param_names to the actual parameter names used in your LoRA implementation\n",
    "test_only_lora_trainable(model, lora_param_names=[\"lora_A\", \"lora_B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Training Loop (Few Steps)\n",
    "# You should see the loss go down\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step >= 5:  # Run for a few steps only\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
